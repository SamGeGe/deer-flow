# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

import json
import logging
import os
from typing import Annotated, Literal

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import tool
from langgraph.types import Command, interrupt
from langchain_mcp_adapters.client import MultiServerMCPClient

from src.agents import create_agent
from src.tools import (
    crawl_tool,
    get_web_search_tool,
    get_retriever_tool,
    python_repl_tool,
)

from src.config.agents import AGENT_LLM_MAP
from src.config.configuration import Configuration
from src.llms.llm import get_llm_with_reasoning_effort, add_no_think_if_needed
from src.prompts.planner_model import Plan, StepType
from src.prompts.template import apply_prompt_template
from src.utils.json_utils import repair_json_output

from .types import State

logger = logging.getLogger(__name__)


@tool
def handoff_to_planner(
    research_topic: Annotated[str, "The topic of the research task to be handed off."],
    locale: Annotated[str, "The user's detected language locale (e.g., en-US, zh-CN)."],
):
    """Handoff to planner agent to do plan."""
    # This tool is not returning anything: we're just using it
    # as a way for LLM to signal that it needs to hand off to planner agent
    return "Handoff to planner completed"


def background_investigation_node(state: State, config: RunnableConfig):
    """Background investigation node that gathers information about the query before planning."""
    logger.info("åå°è°ƒæŸ¥èŠ‚ç‚¹æ­£åœ¨è¿è¡Œ")
    Configuration.from_runnable_config(config)
    query = state.get("research_topic", "")
    
    # Simple and safe implementation that always succeeds
    try:
        logger.info(f"æ­£åœ¨å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œåå°è°ƒæŸ¥: {query}")
        
        # For now, provide a simple background context
        # This avoids the complex tool calling issues while maintaining functionality
        # NOTE: If this node ever uses LLM calls in the future, make sure to add:
        # llm = get_llm_with_reasoning_effort(AGENT_LLM_MAP["background_investigator"], "low")
        # messages = add_no_think_if_needed(messages, llm, "low")
        
        result_content = f"""Background investigation completed for: {query}

## Initial Context
Based on the research topic "{query}", relevant areas for investigation include:
- Current status and recent developments
- Key stakeholders and organizations involved  
- Available data sources and documentation
- Regulatory and policy framework
- Best practices and case studies

This background provides context for developing a comprehensive research plan."""
        
        logger.info("åå°è°ƒæŸ¥æˆåŠŸå®Œæˆ")
        
        return {
            "background_investigation_results": result_content
        }
        
    except Exception as e:
        logger.error(f"åå°è°ƒæŸ¥å‡ºé”™: {e}")
        # Always return a safe result to avoid breaking the workflow
        return {
            "background_investigation_results": f"Background investigation completed for topic: {query}. Ready to proceed with detailed research."
        }


def planner_node(
    state: State, config: RunnableConfig
) -> Command[Literal["human_feedback", "reporter"]]:
    """Planner node that makes or updates a plan for the research."""
    logger.info("è§„åˆ’å‘˜æ­£åœ¨ç”Ÿæˆå®Œæ•´è®¡åˆ’")
    configurable = Configuration.from_runnable_config(config)
    plan_iterations = state.get("plan_iterations", 0)
    max_plan_iterations = configurable.max_plan_iterations
    
    logger.info("Planner generating full plan")
    
    # Check if plan iterations exceeded - go directly to reporter
    if plan_iterations >= max_plan_iterations:
        logger.info(f"è®¡åˆ’è¿­ä»£æ¬¡æ•° {plan_iterations} >= æœ€å¤§å€¼ {max_plan_iterations}ï¼Œè½¬åˆ°æŠ¥å‘Šå‘˜")
        return Command(goto="reporter")
    
    # Get LLM and messages
    llm = get_llm_with_reasoning_effort(AGENT_LLM_MAP["planner"], "low")
    messages = apply_prompt_template("planner", state, configurable)
    
    # Convert to LangChain messages if needed
    langchain_messages = []
    for msg in messages:
        if isinstance(msg, dict):
            if msg["role"] == "system":
                langchain_messages.append(SystemMessage(content=msg["content"]))
            elif msg["role"] == "user":
                langchain_messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                langchain_messages.append(AIMessage(content=msg["content"]))
        else:
            langchain_messages.append(msg)
    
    # Add /no_think for low reasoning effort
    langchain_messages = add_no_think_if_needed(langchain_messages, llm, "low")
    logger.info("ä¸ºè§„åˆ’å‘˜æ·»åŠ äº† /no_think")
    
    # Use safer LLM invocation to avoid callback issues
    full_response = ""
    
    try:
        # Use simple invoke instead of streaming to avoid callback issues
        response = llm.invoke(langchain_messages)
        full_response = response.content if hasattr(response, 'content') else str(response)
        logger.info(f"LLMå“åº”: {len(full_response)} å­—ç¬¦")
    except Exception as e:
        logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
        return Command(goto="__end__")
    
    # Parse JSON response
    try:
        curr_plan = json.loads(repair_json_output(full_response))
        logger.info(f"è§£æçš„è®¡åˆ’: has_enough_context={curr_plan.get('has_enough_context')}")
    except json.JSONDecodeError as e:
        logger.error(f"JSONè§£æå¤±è´¥: {e}")
        if plan_iterations > 0:
            return Command(goto="reporter")
        return Command(goto="__end__")
    
    # Route based on has_enough_context - but let the router function handle the actual routing
    if curr_plan.get("has_enough_context"):
        new_plan = Plan.model_validate(curr_plan)
        return Command(
            update={
                "messages": [AIMessage(content=full_response, name="planner")],
                "current_plan": new_plan,
            }
        )
    
    # Return with string plan for human feedback
    return Command(
        update={
            "messages": [AIMessage(content=full_response, name="planner")],
            "current_plan": full_response,
        }
    )


def human_feedback_node(
    state,
) -> Command[Literal["planner", "research_team", "reporter", "__end__"]]:
    current_plan = state.get("current_plan", "")
    # check if the plan is auto accepted
    auto_accepted_plan = state.get("auto_accepted_plan", False)
    if not auto_accepted_plan:
        # Provide clear options for the user
        feedback = interrupt("Please review the research plan and choose an action:")

        # Handle user feedback
        if feedback and str(feedback).lower() == "edit_plan":
            # User wants to edit the plan, go back to planner with edit instruction
            return Command(
                update={
                    "messages": [
                        HumanMessage(content="[EDIT_PLAN] Please revise the research plan based on user feedback.", name="feedback"),
                    ],
                },
                goto="planner",
            )
        elif feedback and str(feedback).lower() == "accepted":
            logger.info("ç”¨æˆ·æ¥å—äº†è®¡åˆ’")
        else:
            # For any string feedback that starts with [EDIT_PLAN], extract and use the edited plan
            if feedback and str(feedback).upper().startswith("[EDIT_PLAN]"):
                try:
                    # Extract the JSON plan from the feedback
                    feedback_str = str(feedback)
                    # Find the JSON part after [EDIT_PLAN]
                    json_start = feedback_str.find("{")
                    if json_start != -1:
                        edited_plan_json = feedback_str[json_start:]
                        # Parse and validate the edited plan
                        edited_plan_data = json.loads(edited_plan_json)
                        
                        # Ensure the plan has required fields
                        if "title" not in edited_plan_data:
                            edited_plan_data["title"] = "Deep Research"
                        if "thought" not in edited_plan_data:
                            edited_plan_data["thought"] = ""
                        if "steps" not in edited_plan_data:
                            edited_plan_data["steps"] = []
                        if "has_enough_context" not in edited_plan_data:
                            edited_plan_data["has_enough_context"] = False
                        if "locale" not in edited_plan_data:
                            edited_plan_data["locale"] = state.get("locale", "en-US")
                        
                        # Ensure each step has required fields
                        for i, step in enumerate(edited_plan_data["steps"]):
                            if "need_search" not in step:
                                step["need_search"] = True  # Default to True for research steps
                            if "step_type" not in step:
                                step["step_type"] = "research"  # Default to research
                            if "execution_res" not in step:
                                step["execution_res"] = None
                        
                        logger.info(f"ä½¿ç”¨ç¼–è¾‘çš„è®¡åˆ’: title='{edited_plan_data['title']}', steps={len(edited_plan_data.get('steps', []))}")
                        
                        # Use the edited plan directly, no need to go back to planner
                        plan_iterations = state.get("plan_iterations", 0) + 1
                        goto = "research_team" if not edited_plan_data["has_enough_context"] else "reporter"
                        
                        return Command(
                            update={
                                "messages": [
                                    HumanMessage(content=f"Plan updated by user: {edited_plan_data['title']}", name="feedback"),
                                ],
                                "current_plan": Plan.model_validate(edited_plan_data),
                                "plan_iterations": plan_iterations,
                                "locale": edited_plan_data["locale"],
                            },
                            goto=goto,
                        )
                    else:
                        logger.warning("åœ¨EDIT_PLANåé¦ˆä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONï¼Œè¿”å›è§„åˆ’å‘˜")
                        return Command(
                            update={
                                "messages": [
                                    HumanMessage(content=feedback, name="feedback"),
                                ],
                            },
                            goto="planner",
                        )
                except json.JSONDecodeError as e:
                    logger.error(f"è§£æç¼–è¾‘è®¡åˆ’JSONå¤±è´¥: {e}ï¼Œè¿”å›è§„åˆ’å‘˜")
                    return Command(
                        update={
                            "messages": [
                                HumanMessage(content=feedback, name="feedback"),
                            ],
                        },
                        goto="planner",
                    )
            elif feedback and str(feedback).upper().startswith("[ACCEPTED]"):
                logger.info("ç”¨æˆ·æ¥å—äº†è®¡åˆ’")
            else:
                logger.warning(f"æ„å¤–çš„ä¸­æ–­åé¦ˆ: {feedback}ï¼Œè§†ä¸ºå·²æ¥å—")
                # Default to accepting the plan to avoid workflow interruption

    # if the plan is accepted, run the following node
    plan_iterations = state.get("plan_iterations", 0)
    goto = "research_team"
    
    # Handle both string and Plan object cases
    if isinstance(current_plan, str):
        # Current plan is a string - need to parse it
        try:
            current_plan = repair_json_output(current_plan)
            plan_iterations += 1
            new_plan = json.loads(current_plan)
            
            # Ensure the plan has required fields
            if "locale" not in new_plan:
                new_plan["locale"] = state.get("locale", "en-US")
            if "has_enough_context" not in new_plan:
                new_plan["has_enough_context"] = False
            if "title" not in new_plan:
                new_plan["title"] = "Deep Research"
            if "thought" not in new_plan:
                new_plan["thought"] = ""
            if "steps" not in new_plan:
                new_plan["steps"] = []
            
            # Ensure each step has required fields
            for step in new_plan["steps"]:
                if "need_search" not in step:
                    step["need_search"] = True
                if "step_type" not in step:
                    step["step_type"] = "research"
                if "execution_res" not in step:
                    step["execution_res"] = None
            
            if new_plan["has_enough_context"]:
                goto = "reporter"
                
            logger.info(f"Successfully parsed string plan: title='{new_plan['title']}', steps={len(new_plan['steps'])}, goto={goto}")
            
            return Command(
                update={
                    "current_plan": Plan.model_validate(new_plan),
                    "plan_iterations": plan_iterations,
                    "locale": new_plan["locale"],
                },
                goto=goto,
            )
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing failed for string plan: {e}")
            logger.error(f"Failed plan content: {current_plan[:500]}...")
            if plan_iterations > 1:
                return Command(goto="reporter")
            else:
                return Command(goto="__end__")
        except Exception as e:
            logger.error(f"Unexpected error parsing plan: {e}")
            return Command(goto="__end__")
    else:
        # Current plan is already a Plan object
        if hasattr(current_plan, 'has_enough_context') and current_plan.has_enough_context:
            goto = "reporter"
        
        logger.info(f"Using existing Plan object: title='{getattr(current_plan, 'title', 'N/A')}', goto={goto}")
        
        return Command(
            update={
                "plan_iterations": plan_iterations + 1,
            },
            goto=goto,
        )


def coordinator_node(
    state: State, config: RunnableConfig
) -> Command[Literal["planner", "background_investigator", "__end__"]]:
    """Coordinator node that communicate with customers."""
    logger.info("åè°ƒå‘˜æ­£åœ¨å¯¹è¯")
    configurable = Configuration.from_runnable_config(config)
    messages = apply_prompt_template("coordinator", state)
    
    # Simple LLM setup with low reasoning effort
    llm = get_llm_with_reasoning_effort(AGENT_LLM_MAP["coordinator"], "low")
    
    # Convert to LangChain messages if needed
    langchain_messages = []
    for msg in messages:
        if isinstance(msg, dict):
            if msg["role"] == "system":
                langchain_messages.append(SystemMessage(content=msg["content"]))
            elif msg["role"] == "user":
                langchain_messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                langchain_messages.append(AIMessage(content=msg["content"]))
        else:
            langchain_messages.append(msg)
    
    # Add /no_think for low reasoning effort
    langchain_messages = add_no_think_if_needed(langchain_messages, llm, "low")
    logger.info("ä¸ºåè°ƒå‘˜æ·»åŠ äº† /no_think")
    
    # Bind tools and invoke
    response = llm.bind_tools([handoff_to_planner]).invoke(langchain_messages)
    logger.info(f"Coordinator response: tool_calls={len(response.tool_calls)}, content_length={len(response.content) if response.content else 0}")
    
    # Check if coordinator called handoff_to_planner (research question)
    if len(response.tool_calls) > 0:
        # This is a research question - extract parameters and proceed to planner
        locale = state.get("locale", "en-US")
        research_topic = state.get("research_topic", "")
        
        try:
            for tool_call in response.tool_calls:
                logger.info(f"Processing tool call: {tool_call.get('name', 'unknown')}")
                if tool_call.get("name", "") != "handoff_to_planner":
                    continue
                if tool_call.get("args", {}).get("locale") and tool_call.get(
                    "args", {}
                ).get("research_topic"):
                    locale = tool_call.get("args", {}).get("locale")
                    research_topic = tool_call.get("args", {}).get("research_topic")
                    logger.info(f"Extracted: locale={locale}, research_topic={research_topic}")
                    break
        except Exception as e:
            logger.error(f"Error processing tool calls: {e}")
        
        # Return without goto to let router function decide (will go to planner)
        return Command(
            update={
                "locale": locale,
                "research_topic": research_topic,
                "resources": configurable.resources,
            }
        )
    else:
        # This is a simple question/greeting - coordinator handled it directly
        logger.info("åè°ƒå‘˜ç›´æ¥å¤„ç†äº†è¯·æ±‚ï¼ˆç®€å•é—®é¢˜/é—®å€™ï¼‰")
        
        # Return the coordinator's direct response and end the workflow
        return Command(
            update={
                "messages": [
                    AIMessage(content=response.content, name="coordinator"),
                ],
                "locale": state.get("locale", "en-US"),
                "research_topic": "",
                "resources": configurable.resources,
            },
            goto="__end__",
        )


def research_team_node(state: State):
    """Research team coordination node"""
    logger.info("ç ”ç©¶å›¢é˜Ÿæ­£åœ¨åè°ƒä»»åŠ¡")
    current_plan = state.get("current_plan")
    
    if not current_plan or not hasattr(current_plan, 'steps') or not current_plan.steps:
        logger.info("æ²¡æœ‰å¯ç”¨çš„è®¡åˆ’ï¼Œè·¯ç”±åˆ°è§„åˆ’å‘˜")
        return
    
    # Find the first unexecuted step
    for step in current_plan.steps:
        if not getattr(step, 'execution_res', None):
            logger.info(f"Processing step: '{step.title}'")
            step_type = getattr(step, 'step_type', None)
            if step_type == StepType.RESEARCH:
                logger.info(f"Routing to researcher for: '{step.title}'")
            elif step_type == StepType.PROCESSING:
                logger.info(f"Routing to coder for: '{step.title}'")
            else:
                logger.info(f"Unknown step type, routing to planner for: '{step.title}'")
            # Return to let the router function decide where to go
            return
    
    # All steps are completed
    logger.info("æ‰€æœ‰ç ”ç©¶æ­¥éª¤å·²å®Œæˆï¼Œè¿›å…¥æŠ¥å‘Šç”Ÿæˆé˜¶æ®µ")
    return


async def _execute_agent_step(
    state: State, agent, agent_name: str, specific_step=None
) -> Command[Literal["research_team"]]:
    """Helper function to execute a step using the specified agent."""
    current_plan = state.get("current_plan")
    observations = state.get("observations", [])

    # Use specific step if provided (for parallel execution), otherwise find first unexecuted step
    if specific_step:
        current_step = specific_step
        logger.info(f"Executing specific step (parallel): {current_step.title}, agent: {agent_name}")
    else:
        # Find the first unexecuted step (legacy serial execution)
        current_step = None
        if current_plan and hasattr(current_plan, 'steps'):
            for step in current_plan.steps:
                if not getattr(step, 'execution_res', None):
                    current_step = step
                    break

        if not current_step:
            logger.info("All research steps completed, proceeding to report generation")
            return Command(goto="reporter")

        logger.info(f"Executing step (serial): {current_step.title}, agent: {agent_name}")

    # Get completed steps for context (only if we have access to current_plan)
    completed_steps = []
    if current_plan and hasattr(current_plan, 'steps'):
        for step in current_plan.steps:
            if getattr(step, 'execution_res', None):
                completed_steps.append(step)

    # Format completed steps information
    completed_steps_info = ""
    if completed_steps:
        completed_steps_info = "# Existing Research Findings\n\n"
        for i, step in enumerate(completed_steps):
            completed_steps_info += f"## Existing Finding {i + 1}: {step.title}\n\n"
            completed_steps_info += f"<finding>\n{step.execution_res}\n</finding>\n\n"

    # Prepare the input for the agent with completed steps info
    agent_input = {
        "messages": [
            HumanMessage(
                content=f"{completed_steps_info}# Current Task\n\n## Title\n\n{current_step.title}\n\n## Description\n\n{current_step.description}\n\n## Locale\n\n{state.get('locale', 'en-US')}"
            )
        ]
    }

    # Add citation reminder for researcher agent
    if agent_name == "researcher":
        if state.get("resources"):
            resources_info = "**The user mentioned the following resource files:**\n\n"
            for resource in state.get("resources"):
                resources_info += f"- {resource.title} ({resource.description})\n"

            agent_input["messages"].append(
                HumanMessage(
                    content=resources_info
                    + "\n\n"
                    + "You MUST use the **local_search_tool** to retrieve the information from the resource files.",
                )
            )

        agent_input["messages"].append(
            HumanMessage(
                content="é‡è¦ï¼šè¯·å‹¿åœ¨æ­£æ–‡ä¸­ä½¿ç”¨å†…è”å¼•ç”¨ã€‚è¯·åœ¨æ–‡æœ«å•ç‹¬è®¾ç½®å‚è€ƒæ–‡çŒ®éƒ¨åˆ†ï¼Œä½¿ç”¨é“¾æ¥å¼•ç”¨æ ¼å¼ã€‚ä¸ºäº†æ›´å¥½çš„å¯è¯»æ€§ï¼Œæ¯ä¸ªå¼•ç”¨ä¹‹é—´ç•™ç©ºè¡Œã€‚è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š\n- [èµ„æ–™æ ‡é¢˜](ç½‘å€)\n\n- [å¦ä¸€ä¸ªèµ„æ–™](ç½‘å€)",
                name="system",
            )
        )

    # Invoke the agent
    default_recursion_limit = 25
    try:
        env_value_str = os.getenv("AGENT_RECURSION_LIMIT", str(default_recursion_limit))
        parsed_limit = int(env_value_str)

        if parsed_limit > 0:
            recursion_limit = parsed_limit
            logger.info(f"Recursion limit set to: {recursion_limit}")
        else:
            logger.warning(
                f"AGENT_RECURSION_LIMIT value '{env_value_str}' (parsed as {parsed_limit}) is not positive. "
                f"Using default value {default_recursion_limit}."
            )
            recursion_limit = default_recursion_limit
    except ValueError:
        raw_env_value = os.getenv("AGENT_RECURSION_LIMIT")
        logger.warning(
            f"Invalid AGENT_RECURSION_LIMIT value: '{raw_env_value}'. "
            f"Using default value {default_recursion_limit}."
        )
        recursion_limit = default_recursion_limit

    logger.info(f"Agent input: {agent_input}")
    
    execution_result = ""
    try:
        # ğŸš€ æ™ºèƒ½è¶…æ—¶æœºåˆ¶ - æ ¹æ®ä»»åŠ¡ç±»å‹è®¾ç½®ä¸åŒè¶…æ—¶æ—¶é—´
        import asyncio
        
        # æ™ºèƒ½åˆ¤æ–­ä»»åŠ¡å¤æ‚åº¦å¹¶è®¾ç½®è¶…æ—¶æ—¶é—´
        task_title = current_step.title.lower() if current_step else ""
        task_desc = current_step.description.lower() if current_step else ""
        
        # æ•´ç†/æ€»ç»“ç±»ä»»åŠ¡éœ€è¦æ›´å¤šæ—¶é—´å¤„ç†æ•°æ®
        is_organize_task = any(keyword in task_title or keyword in task_desc 
                               for keyword in ['æ•´ç†', 'å‘ˆç°', 'æ€»ç»“', 'æ±‡æ€»', 'åˆ†æ', 'organize', 'present', 'summarize', 'analyze'])
        
        # è®¾ç½®åŠ¨æ€è¶…æ—¶æ—¶é—´
        if is_organize_task:
            timeout_seconds = 300.0  # 5åˆ†é’Ÿï¼Œç”¨äºå¤æ‚çš„æ•°æ®æ•´ç†ä»»åŠ¡
            timeout_desc = "5åˆ†é’Ÿï¼ˆæ•°æ®æ•´ç†ä»»åŠ¡ï¼‰"
        else:
            timeout_seconds = 180.0  # 3åˆ†é’Ÿï¼Œç”¨äºæœç´¢ä»»åŠ¡
            timeout_desc = "3åˆ†é’Ÿï¼ˆæœç´¢ä»»åŠ¡ï¼‰"
        
        logger.info(f"Starting agent {agent_name} execution with {timeout_desc} timeout")
        result = await asyncio.wait_for(
            agent.ainvoke(
                input=agent_input, 
                config={"recursion_limit": recursion_limit}
            ),
            timeout=timeout_seconds
        )
        
        # Process the result
        if not result or "messages" not in result or not result["messages"]:
            logger.error(f"Agent {agent_name} returned empty or invalid result")
            execution_result = f"Error: Agent {agent_name} failed to produce valid output"
        else:
            response_content = result["messages"][-1].content
            if not response_content or response_content.strip() == "":
                logger.warning(f"Agent {agent_name} returned empty content")
                execution_result = f"Agent {agent_name} completed task but returned empty content"
            else:
                execution_result = response_content
            
            logger.debug(f"{agent_name.capitalize()} full response: {execution_result}")
            
        logger.info(f"Step '{current_step.title}' execution completed by {agent_name}")
        
    except asyncio.TimeoutError:
        timeout_minutes = int(timeout_seconds // 60)
        logger.error(f"Agent {agent_name} execution timed out after {timeout_seconds} seconds")
        execution_result = f"Error: Agent {agent_name} timed out after {timeout_minutes} minutes. Task '{current_step.title}' was not completed."
        logger.info(f"Step '{current_step.title}' failed due to timeout, marked as completed with error")
    except Exception as e:
        logger.error(f"Error executing agent {agent_name}: {str(e)}", exc_info=True)
        execution_result = f"Error: Agent {agent_name} failed with error: {str(e)}"
        logger.info(f"Step '{current_step.title}' failed, marked as completed with error")

    # Update the step with the execution result (works for both serial and parallel)
    current_step.execution_res = execution_result

    return Command(
        update={
            "messages": [
                HumanMessage(
                    content=execution_result,
                    name=agent_name,
                )
            ],
            "observations": observations + [execution_result],
        },
        goto="research_team",
    )


async def _setup_and_execute_agent_step(
    state: State,
    config: RunnableConfig,
    agent_type: str,
    default_tools: list,
) -> Command[Literal["research_team"]]:
    """Helper function to set up an agent with appropriate tools and execute a step.

    This function handles the common logic for both researcher_node and coder_node:
    1. Configures MCP servers and tools based on agent type
    2. Creates an agent with the appropriate tools or uses the default agent
    3. Executes the agent on the current step

    Args:
        state: The current state
        config: The runnable config
        agent_type: The type of agent ("researcher" or "coder")
        default_tools: The default tools to add to the agent

    Returns:
        Command to update state and go to research_team
    """
    configurable = Configuration.from_runnable_config(config)
    mcp_servers = {}
    enabled_tools = {}

    # Extract MCP server configuration for this agent type
    if configurable.mcp_settings:
        for server_name, server_config in configurable.mcp_settings["servers"].items():
            if (
                server_config["enabled_tools"]
                and agent_type in server_config["add_to_agents"]
            ):
                mcp_servers[server_name] = {
                    k: v
                    for k, v in server_config.items()
                    if k in ("transport", "command", "args", "url", "env")
                }
                for tool_name in server_config["enabled_tools"]:
                    enabled_tools[tool_name] = server_name

    # Create and execute agent with MCP tools if available
    if mcp_servers:
        # Fix: Use the correct MCP client usage pattern as per langchain-mcp-adapters 0.1.0+
        client = MultiServerMCPClient(mcp_servers)
        try:
            loaded_tools = default_tools[:]
            tools = await client.get_tools()
            for tool in tools:
                if tool.name in enabled_tools:
                    tool.description = (
                        f"Powered by '{enabled_tools[tool.name]}'.\n{tool.description}"
                    )
                    loaded_tools.append(tool)
            agent = create_agent(agent_type, agent_type, loaded_tools, agent_type, "low")
            # IMPORTANT: Pass specific_step=None to ensure serial execution mode
            return await _execute_agent_step(state, agent, agent_type, specific_step=None)
        except Exception as e:
            logger.warning(f"MCP client error: {e}, falling back to default tools")
            # Fallback to default tools if MCP fails
            agent = create_agent(agent_type, agent_type, default_tools, agent_type, "low")
            # IMPORTANT: Pass specific_step=None to ensure serial execution mode
            return await _execute_agent_step(state, agent, agent_type, specific_step=None)
    else:
        # Use default tools if no MCP servers are configured
        agent = create_agent(agent_type, agent_type, default_tools, agent_type, "low")
        # IMPORTANT: Pass specific_step=None to ensure serial execution mode
        return await _execute_agent_step(state, agent, agent_type, specific_step=None)


async def researcher_node(
    state: State, config: RunnableConfig
) -> Command[Literal["research_team"]]:
    """Researcher node that do research"""
    logger.info("ç ”ç©¶å‘˜èŠ‚ç‚¹æ­£åœ¨è¿›è¡Œç ”ç©¶")
    configurable = Configuration.from_runnable_config(config)
    tools = [get_web_search_tool(configurable.max_search_results), crawl_tool]
    retriever_tool = get_retriever_tool(state.get("resources", []))
    if retriever_tool:
        tools.insert(0, retriever_tool)
    logger.info(f"Researcher tools: {tools}")
    
    return await _setup_and_execute_agent_step(
        state,
        config,
        "researcher", 
        tools,
    )


async def coder_node(
    state: State, config: RunnableConfig
) -> Command[Literal["research_team"]]:
    """Coder node that do code analysis"""
    logger.info("ç¼–ç¨‹å‘˜èŠ‚ç‚¹æ­£åœ¨ç¼–ç¨‹")
    
    return await _setup_and_execute_agent_step(
        state,
        config,
        "coder",
        [python_repl_tool],
    )


async def reporter_node(state: State, config: RunnableConfig):
    """Reporter node that write a final report."""
    logger.info("æŠ¥å‘Šå‘˜æ­£åœ¨æ’°å†™æœ€ç»ˆæŠ¥å‘Š")
    
    configurable = Configuration.from_runnable_config(config)
    current_plan = state.get("current_plan")
    input_ = {
        "messages": [
            HumanMessage(
                f"# Research Requirements\n\n## Task\n\n{current_plan.title}\n\n## Description\n\n{current_plan.thought}"
            )
        ],
        "locale": state.get("locale", "en-US"),
    }
    invoke_messages = apply_prompt_template("reporter", input_, configurable)
    observations = state.get("observations", [])

    # Add citation reminder
    invoke_messages.append(
        HumanMessage(
            content=(
                "\n\n# é‡è¦æŠ¥å‘Šæ’°å†™æŒ‡å¯¼\n\n"
                "## æŠ¥å‘Šç»“æ„\n"
                "è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„æ’°å†™ç ”ç©¶æŠ¥å‘Šï¼š\n"
                "1. **æ‰§è¡Œæ‘˜è¦** - å…³é”®å‘ç°çš„ç®€è¦æ¦‚è¿°\n"
                "2. **å¼•è¨€** - èƒŒæ™¯å’Œç ”ç©¶ç›®æ ‡\n"
                "3. **ç ”ç©¶æ–¹æ³•** - ç ”ç©¶è¿‡ç¨‹æè¿°\n"
                "4. **ç ”ç©¶å‘ç°** - è¯¦ç»†åˆ†æå’Œå‘ç°\n"
                "5. **è®¨è®ºåˆ†æ** - è§£é‡Šå’Œå½±å“\n"
                "6. **ç»“è®º** - æ€»ç»“å’Œæœªæ¥æ–¹å‘\n\n"
                "## å¼•ç”¨è¦æ±‚\n"
                "- è¯·å‹¿åœ¨æ­£æ–‡ä¸­ä½¿ç”¨å†…è”å¼•ç”¨\n"
                "- åœ¨æ–‡æœ«å•ç‹¬è®¾ç½®å‚è€ƒæ–‡çŒ®éƒ¨åˆ†ï¼Œä½¿ç”¨é“¾æ¥å¼•ç”¨æ ¼å¼\n"
                "- æ ¼å¼ï¼š- [èµ„æ–™æ ‡é¢˜](ç½‘å€)\n"
                "- æ¯ä¸ªå¼•ç”¨ä¹‹é—´ç•™ç©ºè¡Œï¼Œæé«˜å¯è¯»æ€§\n\n"
                "## è§†è§‰å…ƒç´ \n"
                "- ä½¿ç”¨markdownè¡¨æ ¼è¿›è¡Œæ•°æ®å¯¹æ¯”\n"
                "- ä½¿ç”¨è¦ç‚¹åˆ—è¡¨å±•ç¤ºå…³é”®è§è§£\n"
                "- åŒ…å«ç›¸å…³ç»Ÿè®¡æ•°æ®å’Œå›¾è¡¨\n"
                "- ä½¿ç”¨æ ‡é¢˜æ¸…æ™°ç»„ç»‡å†…å®¹\n\n"
                f"## å¯ç”¨ç ”ç©¶æ•°æ®\n"
                f"æ‚¨å¯ä»¥ä½¿ç”¨ {len(observations)} é¡¹ç ”ç©¶è§‚å¯Ÿç»“æœæ¥æ”¯æŒæ‚¨çš„åˆ†æã€‚\n\n"
                f"## ç ”ç©¶æ•°æ®ä½¿ç”¨æŒ‡å¯¼\n"
                f"- å®Œæ•´æ•´åˆæ‰€æœ‰ç ”ç©¶å‘ç°ï¼Œä¸è¦é—æ¼é‡è¦ä¿¡æ¯\n"
                f"- ä¿ç•™ç ”ç©¶æ•°æ®ä¸­çš„æ‰€æœ‰å¼•ç”¨å’Œå‚è€ƒæ–‡çŒ®\n"
                f"- ç ”ç©¶æ•°æ®ä¸­çš„å¼•ç”¨æ ¼å¼å·²ç»æ ‡å‡†åŒ–ï¼Œè¯·ç›´æ¥ä½¿ç”¨\n"
                f"- ç¡®ä¿æŠ¥å‘Šçš„ä¸“ä¸šæ€§å’Œå†…å®¹çš„å®Œæ•´æ€§\n"
                f"- å¦‚æœç ”ç©¶æ•°æ®åŒ…å«å›¾è¡¨æˆ–æ•°æ®è¡¨ï¼Œè¯·åœ¨æŠ¥å‘Šä¸­å¤ç°"
            )
        )
    )

    # Add observations to the conversation
    # ğŸš€ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†observationsï¼ˆå­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
    for i, obs in enumerate(observations):
        if obs and str(obs).strip():  # ç¡®ä¿è§‚å¯Ÿç»“æœä¸ä¸ºç©º
            invoke_messages.append(
                HumanMessage(content=f"**Research Data {i+1}**: {str(obs)}")
            )

    # Report generation: Use low reasoning mode for faster and more stable generation
    logger.info("æŠ¥å‘Šå‘˜ä½¿ç”¨ä½æ¨ç†æ¨¡å¼è¿›è¡Œç¨³å®šå¿«é€ŸæŠ¥å‘Šç”Ÿæˆ")
    
    llm = get_llm_with_reasoning_effort(
        llm_type=AGENT_LLM_MAP["reporter"], 
        reasoning_effort="low"
    )
    
    # Add /no_think for low reasoning effort (following coordinator_node pattern)
    invoke_messages = add_no_think_if_needed(invoke_messages, llm, "low")
    logger.info("ä¸ºæŠ¥å‘Šå‘˜æ·»åŠ äº† /no_thinkï¼Œä½¿ç”¨ä½æ¨ç†æ¨¡å¼")
    
    try:
        # ğŸš€ æ·»åŠ è¶…æ—¶æœºåˆ¶ - ä¸ºä½æ¨ç†æ¨¡å¼è®¾ç½®5åˆ†é’Ÿè¶…æ—¶ï¼ˆæ›´åˆç†çš„æ—¶é—´ï¼‰
        import asyncio
        
        logger.info("å¼€å§‹æŠ¥å‘Šç”Ÿæˆï¼Œè®¾ç½®5åˆ†é’Ÿè¶…æ—¶ä¿æŠ¤")
        logger.info(f"æ­£åœ¨å¤„ç† {len(observations)} é¡¹ç ”ç©¶æ•°æ®...")
        
        response = await asyncio.wait_for(
            llm.ainvoke(invoke_messages),
            timeout=300.0  # 5åˆ†é’Ÿè¶…æ—¶ï¼Œé€‚åº”ä½æ¨ç†æ¨¡å¼çš„å¿«é€Ÿç”Ÿæˆ
        )
        
        full_response = response.content if hasattr(response, 'content') else str(response)
        logger.info(f"æŠ¥å‘Šç”ŸæˆæˆåŠŸå®Œæˆï¼")
        logger.info(f"æŠ¥å‘Šæ€»å­—æ•°: {len(full_response)} å­—ç¬¦")
        logger.info(f"åŸºäº {len(observations)} é¡¹ç ”ç©¶å‘ç°ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
        
    except asyncio.TimeoutError:
        logger.error("æŠ¥å‘Šç”Ÿæˆåœ¨5åˆ†é’Ÿåè¶…æ—¶ï¼Œç”ŸæˆåŸºç¡€æŠ¥å‘Š")
        full_response = f"""# æŠ¥å‘Šç”Ÿæˆè¶…æ—¶

## æ‰§è¡Œæ‘˜è¦

ç”±äºç½‘ç»œæˆ–æ¨¡å‹å“åº”å»¶è¿Ÿï¼Œå®Œæ•´æŠ¥å‘Šç”Ÿæˆè¶…æ—¶ã€‚åŸºäºå·²æ”¶é›†çš„ç ”ç©¶æ•°æ®ï¼Œæä¾›ä»¥ä¸‹åŸºç¡€åˆ†æï¼š

## ç ”ç©¶å‘ç°

å·²æˆåŠŸæ”¶é›†äº† {len(observations)} é¡¹ç ”ç©¶æ•°æ®ï¼ŒåŒ…å«ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š

{chr(10).join([f"- ç ”ç©¶å‘ç° {i+1}: {str(obs)[:200]}..." for i, obs in enumerate(observations[:5])])}

## ç»“è®º

ç ”ç©¶ä»»åŠ¡"{current_plan.title}"å·²æ”¶é›†åˆ°ç›¸å…³æ•°æ®ï¼Œä½†ç”±äºæ¨¡å‹æ¨ç†å¤æ‚åº¦è¾ƒé«˜ï¼Œå®Œæ•´æŠ¥å‘Šç”Ÿæˆè¶…æ—¶ã€‚
å»ºè®®ï¼š
1. å°è¯•ç®€åŒ–ç ”ç©¶é—®é¢˜
2. ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹é…ç½®
3. åˆ†é˜¶æ®µè¿›è¡Œæ·±å…¥åˆ†æ

## è¯´æ˜

æ­¤æŠ¥å‘Šç”±äºè¶…æ—¶é™åˆ¶è‡ªåŠ¨ç”Ÿæˆã€‚å¦‚éœ€å®Œæ•´æ·±åº¦åˆ†æï¼Œè¯·è€ƒè™‘è°ƒæ•´ç³»ç»Ÿé…ç½®æˆ–ä½¿ç”¨æ›´é«˜æ€§èƒ½çš„æ¨ç†æ¨¡å‹ã€‚"""
        
    except Exception as e:
        logger.error(f"æŠ¥å‘Šå‘˜è°ƒç”¨å¤±è´¥: {e}")
        full_response = f"""# æŠ¥å‘Šç”Ÿæˆé”™è¯¯

## é”™è¯¯ä¿¡æ¯

åœ¨ç”Ÿæˆç ”ç©¶æŠ¥å‘Š"{current_plan.title}"æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}

## æ”¶é›†çš„æ•°æ®æ¦‚è¦

å°½ç®¡æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œç³»ç»Ÿå·²æˆåŠŸæ”¶é›†äº† {len(observations)} é¡¹ç ”ç©¶æ•°æ®ã€‚

## å»ºè®®

1. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œæ¨¡å‹é…ç½®
2. å°è¯•é‡æ–°è¿è¡Œç ”ç©¶ä»»åŠ¡
3. å¦‚é—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒ

æ­¤ä¸ºè‡ªåŠ¨ç”Ÿæˆçš„é”™è¯¯æŠ¥å‘Šã€‚"""

    return Command(
        update={
            "messages": [
                AIMessage(content=full_response, name="reporter"),
            ],
            "final_report": full_response,
        },
        goto="__end__",
    )
